{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c9b6969-1f6e-49aa-a5e4-71c92baf0413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.2+cu111\n",
      "11.1\n",
      "8005\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.backends.cudnn.version())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5008edc8-7d33-4c2a-ad86-a87a15aa3f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import models\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F  # 添加此行以使用 F.interpolate\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from PIL.Image import Resampling  # 导入Resampling\n",
    "\n",
    "# 设置打印选项，显示完整 Tensor 内容\n",
    "torch.set_printoptions(threshold=100000)\n",
    "\n",
    "\n",
    "class UpperBodyDataset(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir, img_transform=None, mask_transform=None, color_to_class=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.img_transform = img_transform\n",
    "        self.mask_transform = mask_transform\n",
    "        self.color_to_class = color_to_class\n",
    "        self.images = [f for f in os.listdir(img_dir) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        mask_name = os.path.splitext(img_name)[0] + '.png'  # 确保mask文件名对应\n",
    "        mask_path = os.path.join(self.mask_dir, mask_name)\n",
    "\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        mask = Image.open(mask_path).convert('RGB')  # 保持三通道\n",
    "\n",
    "        if self.img_transform:\n",
    "            image = self.img_transform(image)\n",
    "        if self.mask_transform:\n",
    "            mask = self.mask_transform(mask)\n",
    "            # print('transform')\n",
    "            \n",
    "        # print(mask)    \n",
    "\n",
    "        # 将掩码从RGB转换为类别索引\n",
    "        mask = self.rgb_to_class(mask)\n",
    "\n",
    "        return image, mask  # 返回形状为 [C, H, W] 和 [H, W]\n",
    "\n",
    "    def rgb_to_class(self, mask):\n",
    "        mask = mask.permute(1, 2, 0).numpy()  # 转换为 H x W x C\n",
    "        h, w, c = mask.shape\n",
    "        class_mask = np.zeros((h, w), dtype=np.int64)  # 将 np.long 替换为 np.int64\n",
    "\n",
    "        for color, class_idx in self.color_to_class.items():\n",
    "        # 创建布尔掩码\n",
    "            matches = np.all(mask == color, axis=-1)\n",
    "            class_mask[matches] = class_idx\n",
    "\n",
    "    # 检查是否有未映射的像素\n",
    "        unique_unmapped = np.unique(class_mask)\n",
    "        if 0 in unique_unmapped and 0 not in self.color_to_class.values():\n",
    "            print(\"警告: 存在未映射的颜色，类别索引为0的像素可能被错误地标记为背景。\")\n",
    "            \n",
    "\n",
    "        class_mask = torch.from_numpy(class_mask).long()  # 转换为 torch.Tensor\n",
    "        \n",
    "        # print(class_mask)\n",
    "        return class_mask\n",
    "\n",
    "class ToInteger(object):\n",
    "    \"\"\"将图像转换为整数（uint8）。\"\"\"\n",
    "    def __call__(self, img):\n",
    "        return np.array(img).astype(np.uint8)\n",
    "\n",
    "class ToTensorWithoutNormalization(object):\n",
    "    def __call__(self, image):\n",
    "        # 将 PIL.Image 转换为 tensor，且不进行归一化\n",
    "        return torch.from_numpy(np.array(image)).permute(2, 0, 1).float()\n",
    "\n",
    "\n",
    "\n",
    "def get_transforms(img_size_h, img_size_w):\n",
    "    img_transform = transforms.Compose([\n",
    "        transforms.Resize((img_size_h, img_size_w), interpolation=Resampling.BILINEAR),\n",
    "        ToTensorWithoutNormalization(),\n",
    "        ToInteger(),\n",
    "        # transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    mask_transform = transforms.Compose([\n",
    "        transforms.Resize((img_size_h, img_size_w), interpolation=Resampling.NEAREST),\n",
    "        ToTensorWithoutNormalization(),\n",
    "    ])\n",
    "    \n",
    "    return img_transform, mask_transform\n",
    "\n",
    "\n",
    "def calculate_metrics(outputs, targets):\n",
    "    \"\"\"计算IoU和像素准确率\"\"\"\n",
    "    predictions = torch.argmax(outputs, dim=1)\n",
    "    predictions = predictions.view(-1)\n",
    "    targets = targets.view(-1)\n",
    "\n",
    "    intersection = (predictions * targets).sum()\n",
    "    union = predictions.sum() + targets.sum() - intersection\n",
    "    iou = (intersection + 1e-6) / (union + 1e-6)\n",
    "\n",
    "    accuracy = (predictions == targets).float().mean()\n",
    "    return iou.item(), accuracy.item()\n",
    "\n",
    "\n",
    "def visualize_prediction(image, mask, prediction, epoch, idx, save_dir, class_colors=None):\n",
    "    \"\"\"可视化预测结果\"\"\"\n",
    "    image = image.cpu().numpy().transpose(1, 2, 0)\n",
    "    image = (image * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406])  # 反标准化\n",
    "    image = np.clip(image, 0, 1)\n",
    "\n",
    "    mask = mask.cpu().numpy()\n",
    "    prediction = torch.argmax(prediction, dim=0).cpu().numpy()\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(image)\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(132)\n",
    "    if class_colors:\n",
    "        mask_color = decode_segmap(mask, class_colors)\n",
    "        plt.imshow(mask_color)\n",
    "    else:\n",
    "        plt.imshow(mask, cmap='gray')\n",
    "    plt.title('Ground Truth')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(133)\n",
    "    if class_colors:\n",
    "        prediction_color = decode_segmap(prediction, class_colors)\n",
    "        plt.imshow(prediction_color)\n",
    "    else:\n",
    "        plt.imshow(prediction, cmap='gray')\n",
    "    plt.title('Prediction')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.savefig(os.path.join(save_dir, f'epoch_{epoch}_sample_{idx}.png'))\n",
    "    plt.close()\n",
    "\n",
    "def decode_segmap(image, class_colors):\n",
    "    \"\"\"\n",
    "    将类别索引图转换为RGB图像。\n",
    "    \"\"\"\n",
    "    r = np.zeros_like(image).astype(np.uint8)\n",
    "    g = np.zeros_like(image).astype(np.uint8)\n",
    "    b = np.zeros_like(image).astype(np.uint8)\n",
    "\n",
    "    for class_idx, color in class_colors.items():\n",
    "        print(class_idx)\n",
    "        print(color)\n",
    "        r[image == class_idx] = color[0]\n",
    "        g[image == class_idx] = color[1]\n",
    "        b[image == class_idx] = color[2]\n",
    "\n",
    "    rgb = np.stack([r, g, b], axis=2)\n",
    "    return rgb\n",
    "\n",
    "\n",
    "class LightweightASPP(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        # Reduce input channels for memory efficiency\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 1)\n",
    "\n",
    "        # Lightweight atrous convolutions\n",
    "        self.aspp1 = nn.Conv2d(out_channels, out_channels, 1)\n",
    "        self.aspp2 = nn.Conv2d(out_channels, out_channels, 3, padding=6, dilation=6, groups=out_channels)\n",
    "        self.aspp3 = nn.Conv2d(out_channels, out_channels, 3, padding=12, dilation=12, groups=out_channels)\n",
    "\n",
    "        # Global context\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.global_conv = nn.Conv2d(in_channels, out_channels, 1)\n",
    "\n",
    "        # Final 1x1 conv\n",
    "        self.final_conv = nn.Conv2d(out_channels * 4, out_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        size = x.size()[2:]\n",
    "\n",
    "        conv1 = self.conv1(x)\n",
    "\n",
    "        aspp1 = self.aspp1(conv1)\n",
    "        aspp2 = self.aspp2(conv1)\n",
    "        aspp3 = self.aspp3(conv1)\n",
    "\n",
    "        # Global features\n",
    "        global_features = self.global_avg_pool(x)\n",
    "        global_features = self.global_conv(global_features)\n",
    "        global_features = F.interpolate(global_features, size=size, mode='bilinear', align_corners=True)\n",
    "\n",
    "        # Concatenate all features\n",
    "        out = torch.cat([aspp1, aspp2, aspp3, global_features], dim=1)\n",
    "        out = self.final_conv(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class LightweightDeepLabv3(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "\n",
    "        # Use a lightweight custom backbone\n",
    "        self.backbone = nn.Sequential(\n",
    "            # Initial conv layer\n",
    "            nn.Conv2d(3, 32, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # Depthwise separable convolutions\n",
    "            self._make_separable_block(32, 64, stride=2),\n",
    "            self._make_separable_block(64, 128, stride=2),\n",
    "            self._make_separable_block(128, 256, stride=1),\n",
    "        )\n",
    "\n",
    "        self.aspp = LightweightASPP(256, 256)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, num_classes, 1)\n",
    "        )\n",
    "\n",
    "    def _make_separable_block(self, in_channels, out_channels, stride):\n",
    "        return nn.Sequential(\n",
    "            # Depthwise\n",
    "            nn.Conv2d(in_channels, in_channels, 3, stride=stride, padding=1, groups=in_channels),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.ReLU(),\n",
    "            # Pointwise\n",
    "            nn.Conv2d(in_channels, out_channels, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_size = x.size()[2:]\n",
    "\n",
    "        # Extract features\n",
    "        features = self.backbone(x)\n",
    "\n",
    "        # Apply ASPP\n",
    "        aspp_features = self.aspp(features)\n",
    "\n",
    "        # Decode and upsample\n",
    "        out = self.decoder(aspp_features)\n",
    "        out = F.interpolate(out, size=input_size, mode='bilinear', align_corners=True)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "def train_model(config):\n",
    "    # 创建保存目录\n",
    "    os.makedirs(config['save_dir'], exist_ok=True)\n",
    "    os.makedirs(os.path.join(config['save_dir'], 'visualizations'), exist_ok=True)\n",
    "\n",
    "    # 设置TensorBoard\n",
    "    writer = SummaryWriter(os.path.join(config['save_dir'], 'logs'))\n",
    "\n",
    "    # 获取图像和掩码的变换\n",
    "    img_transform, mask_transform = get_transforms(config['img_size_h'], config['img_size_w'])\n",
    "\n",
    "    # 创建数据集和数据加载器\n",
    "    train_dataset = UpperBodyDataset(\n",
    "        config['train_img_dir'],\n",
    "        config['train_mask_dir'],\n",
    "        img_transform=img_transform,\n",
    "        mask_transform=mask_transform,\n",
    "        color_to_class=config['color_to_class']  # 传递颜色映射\n",
    "    )\n",
    "    val_dataset = UpperBodyDataset(\n",
    "        config['val_img_dir'],\n",
    "        config['val_mask_dir'],\n",
    "        img_transform=img_transform,\n",
    "        mask_transform=mask_transform,\n",
    "        color_to_class=config['color_to_class']  # 传递颜色映射\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=True,\n",
    "        pin_memory=True if config['device'] == 'cuda' else False,  # 如果使用GPU，启用pin_memory\n",
    "        num_workers=2  # 根据系统配置调整\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=False,\n",
    "        pin_memory=True if config['device'] == 'cuda' else False,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    # 初始化模型\n",
    "    model = LightweightDeepLabv3(num_classes=config['num_classes'])  # 修正参数名称\n",
    "    print(model)\n",
    "    model = model.to(config['device'])\n",
    "\n",
    "    # 损失函数和优化器\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
    "                                                           factor=0.1, patience=5)\n",
    "\n",
    "    # 初始化混合精度Scaler\n",
    "    # scaler = GradScaler()\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(config['num_epochs']):\n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_iou = 0\n",
    "        train_acc = 0\n",
    "\n",
    "        train_bar = tqdm(train_loader, desc=f'Training Epoch {epoch + 1}/{config[\"num_epochs\"]}')\n",
    "        for batch_idx, (images, masks) in enumerate(train_bar):\n",
    "            images = images.to(config['device'], non_blocking=True)\n",
    "            masks = masks.to(config['device'], non_blocking=True)\n",
    "           \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            #             with autocast():  # 启用自动混合精度\n",
    "            outputs = model(images)  # 修正输出获取方式\n",
    "            loss = criterion(outputs, masks)  # masks 已经是 [batch_size, H, W] 且 dtype 为 long\n",
    "\n",
    "            #             scaler.scale(loss).backward()\n",
    "            #             scaler.step(optimizer)\n",
    "            #             scaler.update()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 计算指标\n",
    "            batch_iou, batch_acc = calculate_metrics(outputs, masks)\n",
    "            train_loss += loss.item()\n",
    "            train_iou += batch_iou\n",
    "            train_acc += batch_acc\n",
    "\n",
    "            # 更新进度条\n",
    "            train_bar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'iou': f'{batch_iou:.4f}',\n",
    "                'acc': f'{batch_acc:.4f}'\n",
    "            })\n",
    "\n",
    "            # 可视化第一个batch的第一张图片\n",
    "            if batch_idx == 0:\n",
    "                visualize_prediction(\n",
    "                    images[0], masks[0],\n",
    "                    outputs[0],\n",
    "                    epoch + 1, batch_idx,\n",
    "                    os.path.join(config['save_dir'], 'visualizations')\n",
    "                )\n",
    "\n",
    "        # 计算平均训练指标\n",
    "        train_loss /= len(train_loader)\n",
    "        train_iou /= len(train_loader)\n",
    "        train_acc /= len(train_loader)\n",
    "\n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_iou = 0\n",
    "        val_acc = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_bar = tqdm(val_loader, desc='Validation')\n",
    "            for images, masks in val_bar:\n",
    "                images = images.to(config['device'], non_blocking=True)\n",
    "                masks = masks.to(config['device'], non_blocking=True)\n",
    "\n",
    "                # with autocast():  # 也可以在验证阶段使用 autocast\n",
    "                outputs = model(images)  # 修正输出获取方式\n",
    "                loss = criterion(outputs, masks)\n",
    "\n",
    "                batch_iou, batch_acc = calculate_metrics(outputs, masks)\n",
    "                val_loss += loss.item()\n",
    "                val_iou += batch_iou\n",
    "                val_acc += batch_acc\n",
    "\n",
    "                val_bar.set_postfix({\n",
    "                    'loss': f'{loss.item():.4f}',\n",
    "                    'iou': f'{batch_iou:.4f}',\n",
    "                    'acc': f'{batch_acc:.4f}'\n",
    "                })\n",
    "\n",
    "        # 计算平均验证指标\n",
    "        val_loss /= len(val_loader)\n",
    "        val_iou /= len(val_loader)\n",
    "        val_acc /= len(val_loader)\n",
    "\n",
    "        # 记录到TensorBoard\n",
    "        writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "        writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "        writer.add_scalar('IoU/train', train_iou, epoch)\n",
    "        writer.add_scalar('IoU/val', val_iou, epoch)\n",
    "        writer.add_scalar('Accuracy/train', train_acc, epoch)\n",
    "        writer.add_scalar('Accuracy/val', val_acc, epoch)\n",
    "\n",
    "        # 打印训练信息\n",
    "        print(f'\\nEpoch {epoch + 1}/{config[\"num_epochs\"]}:')\n",
    "        print(f'Train Loss: {train_loss:.4f}, Train IoU: {train_iou:.4f}, Train Acc: {train_acc:.4f}')\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val IoU: {val_iou:.4f}, Val Acc: {val_acc:.4f}\\n')\n",
    "        \n",
    "        visualize_prediction(\n",
    "            images[0], masks[0],\n",
    "            outputs[0],\n",
    "            epoch + 1, batch_idx,\n",
    "            os.path.join(config['save_dir'], 'visualizations'),\n",
    "            class_colors=config['class_colors']\n",
    "        )\n",
    "\n",
    "        # 保存最佳模型\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'val_iou': val_iou,\n",
    "                'val_acc': val_acc\n",
    "            }, os.path.join(config['save_dir'], 'model_deeplabv3_label_test.pth'))\n",
    "\n",
    "        # 学习率调整\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # 清理显存缓存\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # 监控显存使用情况\n",
    "        if config['device'] == 'cuda':\n",
    "            allocated = torch.cuda.memory_allocated(config['device']) / 1024 ** 2\n",
    "            reserved = torch.cuda.memory_reserved(config['device']) / 1024 ** 2\n",
    "            print(f\"Epoch {epoch + 1} - Memory Allocated: {allocated:.2f} MB\")\n",
    "            print(f\"Epoch {epoch + 1} - Memory Reserved: {reserved:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b550c071-13af-4dee-b9ee-327522f59f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-26 13:09:10.184427: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-26 13:09:10.190849: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-26 13:09:10.556863: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-26 13:09:10.659523: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1735189750.862718     524 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1735189750.890618     524 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-26 13:09:11.289876: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/conda/lib/python3.9/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightweightDeepLabv3(\n",
      "  (backbone): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (aspp): LightweightASPP(\n",
      "    (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (aspp1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (aspp2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6), groups=256)\n",
      "    (aspp3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), groups=256)\n",
      "    (global_avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (global_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (final_conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/50:   0%|          | 0/681 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Byte but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_524/2255672792.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# 启动训练\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_524/1812752038.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m             \u001b[0;31m#             with autocast():  # 启用自动混合精度\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 修正输出获取方式\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# masks 已经是 [batch_size, H, W] 且 dtype 为 long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_524/1812752038.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;31m# Extract features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;31m# Apply ASPP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    393\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 395\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    396\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Byte but found Float"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "        # 示例配置\n",
    "        # 定义颜色到类别索引的映射\n",
    "    COLOR_TO_CLASS = {\n",
    "        (0,0,0):0, #-> Background\n",
    "        (1,1,1):1, #-> Hair\n",
    "        (4,4,4):2, #-> Upclothes\n",
    "        (5,5,5):3, #-> Left-shoe \n",
    "        (6,6,6):4, #-> Right-shoe\n",
    "        (7,7,7):5, #-> Noise\n",
    "        (8,8,8):6, #-> Pants\n",
    "        (9,9,9):7, #-> Left_leg\n",
    "        (10,10,10):8, #-> Right_leg\n",
    "        (11,11,11):9,# -> Left_arm\n",
    "        (12,12,12):10,# -> Face\n",
    "        (13,13,13):11# -> Right_arm\n",
    "    # 添加更多类别颜色映射\n",
    "    # (R, G, B): 类别索引,\n",
    "    }\n",
    "    # 反转映射，得到类别索引到颜色的映射\n",
    "    CLASS_TO_COLOR = {\n",
    "        0:(0,0,0), #-> Background\n",
    "        1:(1,1,1), #-> Hair\n",
    "        2:(4,4,4), #-> Upclothes\n",
    "        3:(5,5,5),#-> Left-shoe \n",
    "        4:(6,6,6),#-> Right-shoe\n",
    "        5:(7,7,7),#-> Noise\n",
    "        6:(8,8,8),#-> Pants\n",
    "        7:(9,9,9),#-> Left_leg\n",
    "        8:(10,10,10), #-> Right_leg\n",
    "        9:(11,11,11),# -> Left_arm\n",
    "        10:(12,12,12),# -> Face\n",
    "        11:(13,13,13)# -> Right_arm\n",
    "    }\n",
    "    config = {\n",
    "        'train_img_dir': 'train_img',          # 替换为训练图片文件夹路径\n",
    "        'train_mask_dir': 'train_label',       # 替换为训练mask文件夹路径\n",
    "        'val_img_dir': 'test_img',              # 替换为验证图片文件夹路径\n",
    "        'val_mask_dir': 'test_label',           # 替换为验证mask文件夹路径\n",
    "        'img_size_h': 256,                      # 设置图像高度，调整为128\n",
    "        'img_size_w': 192,                      # 设置图像宽度，调整为128\n",
    "        'batch_size': 2,                        # 设置批量大小为2，尽量减少\n",
    "        'learning_rate': 1e-4,                  # 设置学习率\n",
    "        'num_epochs': 50,                       # 设置训练轮数\n",
    "        'save_dir': 'save_model',               # 替换为模型和日志保存路径\n",
    "       \n",
    "        'color_to_class': COLOR_TO_CLASS,      # 添加颜色到类别映射\n",
    "        'class_colors': CLASS_TO_COLOR,        # 添加用于可视化的颜色映射\n",
    "        'device': 'cpu',  # 设置设备\n",
    "        'num_classes': 12                        # 设置类别数（例如：背景+目标）\n",
    "    }\n",
    "\n",
    "    # 启动训练\n",
    "    train_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fd4888-85e3-450a-b8fe-d939891a5d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
